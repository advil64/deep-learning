### Transposed Convolution

- It's the opposite of a normal convolution where you decompress values basically
- Actually ignore what I just said I have no idea what this shit is
- Weighted convolutions are learnable weights where each element of the 3x3 convolution has a specific weight
- Transconv are made into sparse matrices not sure how though

### Dilated Convolution
- You insert some space between kernel elements which means that the neuron summarizes a larger receptive field

### Recurrent Neural Network
- Used to model sequences in the real world
- Google this shit cause I have no idea what the hell I'm listening to
- N Gram models have something to do with joint probabilities
- Latent variable models approximates probability of xt using a conditional probability of the previous history basically
- 