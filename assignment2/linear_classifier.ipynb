{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill your name and NetID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Advith Chegu\"\n",
    "NET_ID = \"ac1771\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Classifiers\n",
    "\n",
    "In this notebook, you will build a linear image classifier from scratch in PyTorch. You will implement both the forward pass and backward pass of the linear classifier _without_ using PyTorch's autograd capabilities. It is largely adapted (with permission) from https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019.\n",
    "\n",
    "As in the first assignment, you will see code blocks that look like this:\n",
    "```python\n",
    "###############################################################################\n",
    "# TODO: Create a variable x with value 3.7\n",
    "###############################################################################\n",
    "pass\n",
    "# END OF YOUR CODE\n",
    "```\n",
    "\n",
    "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
    "```python\n",
    "###############################################################################\n",
    "# TODO: Create a variable x with value 3.7\n",
    "###############################################################################\n",
    "x = 3.7\n",
    "# END OF YOUR CODE\n",
    "```\n",
    "\n",
    "Also, please remember:\n",
    "- Do not write or modify any code outside of code blocks\n",
    "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
    "- Run all cells before submitting. You will only get credit for code that has been run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install starter code\n",
    "We have implemented some utility functions for this exercise in the [`coutils` package](https://github.com/yfw/starter-code). Run this cell to download and install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/yfw/starter-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "squares",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup code\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import coutils\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "correct_squares",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "squares_invalid_input",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we will load CIFAR10 dataset, with normalization. The CIFAR10 dataset consists of a collection of 60,000 32x32 images from 10 different classes. The 10 different classes are: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. Given an image, our goal here is to predict the class that the image belongs to.\n",
    "\n",
    "In this notebook we will use the **bias trick**: By adding an extra constant feature of ones to each image, we avoid the need to keep track of a bias vector; the bias will be encoded as the part of the weight matrix that interacts with the constant ones in the input.\n",
    "\n",
    "For our linear classifier, we flatten the image into a single long vector. Therefore, each image is represented by a $(32x32x3) + 1 = 3073$ dimension vector (32x32 for height x width, 3 color channels, and 1 for the bias trick)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(validation_ratio = 0.02):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier.\n",
    "  \"\"\"\n",
    "  X_train, y_train, X_test, y_test = coutils.data.cifar10()\n",
    "\n",
    "  # Move all the data to the GPU\n",
    "  X_train = X_train.cuda()\n",
    "  y_train = y_train.cuda()\n",
    "  X_test = X_test.cuda()\n",
    "  y_test = y_test.cuda()\n",
    "\n",
    "  # 0. Visualize some examples from the dataset.\n",
    "  class_names = [\n",
    "      'plane', 'car', 'bird', 'cat', 'deer',\n",
    "      'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "  ]\n",
    "  img = coutils.utils.visualize_dataset(X_train, y_train, 12, class_names)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  # 1. Normalize the data: subtract the mean RGB (zero mean)\n",
    "  mean_image = X_train.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "  X_train -= mean_image\n",
    "  X_test -= mean_image\n",
    "\n",
    "  # 2. Reshape the image data into rows\n",
    "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "  X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "  # 3. Add bias dimension and transform into columns\n",
    "  ones_train = torch.ones(X_train.shape[0], 1, device=X_train.device)\n",
    "  X_train = torch.cat([X_train, ones_train], dim=1)\n",
    "  ones_test = torch.ones(X_test.shape[0], 1, device=X_test.device)\n",
    "  X_test = torch.cat([X_test, ones_test], dim=1)\n",
    "\n",
    "  # 4. Carve out part of the training set to use for validation.\n",
    "  # For random permutation, you can use torch.randperm or torch.randint\n",
    "  # But, for this homework, we use slicing instead.\n",
    "  num_training = int( X_train.shape[0] * (1.0 - validation_ratio) )\n",
    "  num_validation = X_train.shape[0] - num_training\n",
    "\n",
    "  # Return the dataset as a dictionary\n",
    "  data_dict = {}\n",
    "  data_dict['X_val'] = X_train[num_training:num_training + num_validation]\n",
    "  data_dict['y_val'] = y_train[num_training:num_training + num_validation]\n",
    "  data_dict['X_train'] = X_train[0:num_training]\n",
    "  data_dict['y_train'] = y_train[0:num_training]\n",
    "\n",
    "  data_dict['X_test'] = X_test\n",
    "  data_dict['y_test'] = y_test\n",
    "  return data_dict\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "data_dict = get_CIFAR10_data()\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following function to check our gradient implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-7):\n",
    "  \"\"\"\n",
    "  Utility function to perform numeric gradient checking. We use the centered\n",
    "  difference formula to compute a numeric derivative:\n",
    "  \n",
    "  f'(x) =~ (f(x + h) - f(x - h)) / (2h)\n",
    "\n",
    "  Rather than computing a full numeric gradient, we sparsely sample a few\n",
    "  dimensions along which to compute numeric derivatives.\n",
    "\n",
    "  Inputs:\n",
    "  - f: A function that inputs a torch tensor and returns a torch scalar\n",
    "  - x: A torch tensor giving the point at which to evaluate the numeric gradient\n",
    "  - analytic_grad: A torch tensor giving the analytic gradient of f at x\n",
    "  - num_checks: The number of dimensions along which to check\n",
    "  - h: Step size for computing numeric derivatives\n",
    "  \"\"\"\n",
    "  # fix random seed for \n",
    "  coutils.utils.fix_random_seed()\n",
    "\n",
    "  for i in range(num_checks):\n",
    "    \n",
    "    ix = tuple([random.randrange(m) for m in x.shape])\n",
    "    \n",
    "    oldval = x[ix].item()\n",
    "    x[ix] = oldval + h # increment by h\n",
    "    fxph = f(x).item() # evaluate f(x + h)\n",
    "    x[ix] = oldval - h # increment by h\n",
    "    fxmh = f(x).item() # evaluate f(x - h)\n",
    "    x[ix] = oldval     # reset\n",
    "\n",
    "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "    grad_analytic = analytic_grad[ix]\n",
    "    rel_error_top = abs(grad_numerical - grad_analytic)\n",
    "    rel_error_bot = (abs(grad_numerical) + abs(grad_analytic) + 1e-12)\n",
    "    rel_error = rel_error_top / rel_error_bot\n",
    "    msg = 'numerical: %f analytic: %f, relative error: %e'\n",
    "    print(msg % (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier with Softmax\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our linear classifier, we use a simple linear mapping followed by a softmax operation to predict the probability distribution over the different classes. We then use the cross-entropy loss to compare our prediction to the given ground truth label.\n",
    "\n",
    "Assume we are given a training dataset of images $x^{(i)} \\in R^D$, each with an associated class label $y^{(i)}$. Here $i=1...N$ and $y_i \\in 1...K$ where $N$ is the total number of training examples and $K$ is the total number of classes. For our CIFAR10 dataset, we have $N=49000$, $K=10$, and $D=(32x32x3) + 1 = 3073$.\n",
    "\n",
    "The linear classifier multiples the input $x^{(i)}$ with a single parameter matrix $W \\in R^{DxK}$ to predict the probability distribution over classes:\n",
    "\n",
    "$$ o^{(i)} = W^T x^{(i)} $$\n",
    "$$ \\hat{y}^{(i)} = \\text{softmax}(o^{(i)}) $$\n",
    "\n",
    "where the $j$-th entry of $\\hat{y}^{(i)}$ is given by\n",
    "\n",
    "$$ \\hat{y}^{(i)}_j = \\frac{\\exp(o^{(i)}_j)}{\\sum_k \\exp(o^{(i)}_k)} $$  \n",
    "\n",
    "The intermediate values $o^{(i)}$ are the unnormalized log probabilities of the different classes and are commonly referred to as \"logits\". The softmax operation, which we saw in the previous assignment, is used to normalize the logits to a proper probability distribution.\n",
    "\n",
    "We then use the cross-entropy loss to compare the prediction to the ground truth label. Let us first define $p^{(i)}$ to be the ground truth probability distribution of the class. This just means that $p^{(i)}$ is a one-hot vector with a 1 in the $y^{(i)}$-th position and zeros everywhere else. Then, from the definition of cross-entropy we saw in class, we have the following loss for the $i$-th training example:\n",
    "\n",
    "\\begin{align}\n",
    "L^{(i)} &= -\\sum_j p^{(i)}_j \\log \\hat{y}^{(i)}_j \\\\\n",
    "&= - \\log \\hat{y}^{(i)}_{y^{(i)}} \\\\\n",
    "&= - \\log \\frac{\\exp(o^{(i)}_{y^{(i)}})}{\\sum_k \\exp(o^{(i)}_k)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The second equality holds because $p^{(i)}$ has zeros everywhere except position $y^{(i)}$, so we only need to keep the $y^{(i)}$-th position of $\\hat{y}^{(i)}$. Averaging over all the training examples and adding an L2-regularization term, we have the following loss:\n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_i L^{(i)} + \\lambda \\sum_k \\sum_l W_{k,l}^2 $$\n",
    "\n",
    "Where $\\lambda$ is a hyperparameter. Note that for simplicity we do not divide $\\lambda$ by 2 here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start from implementing the naive softmax cross-entropy loss function described above using nested loops. Your solution should have one loop going through all the training examples and more loops going through the number of classes. Your function should return both $L$ and $dL/dW$, the gradient of $L$ with respect to $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5a37cfd98f4e41a35a9170e67f912fa",
     "grade": false,
     "grade_id": "cell-4b40f43c21f21c30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, naive implementation (with loops).  When you implement \n",
    "  the regularization over W, please DO NOT multiply the regularization term by \n",
    "  1/2 (no coefficient). \n",
    "\n",
    "  Inputs have dimension D, there are K classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A PyTorch tensor of shape (D, K) containing weights.\n",
    "  - X: A PyTorch tensor of shape (N, D) containing a minibatch of data.\n",
    "  - y: A PyTorch tensor of shape (N,) containing the ground truth training labels; y[i] = k means\n",
    "    that X[i] has label k, where 0 <= k < K.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an tensor of same shape as W\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = torch.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  pass\n",
    "  # END OF YOUR CODE\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check to see whether we have implemented the loss correctly, run the softmax classifier with a small random weight matrix and no regularization. You should see loss near log(10) = 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "664f94340f2b56e62da8eb9bafc8a37b",
     "grade": true,
     "grade_id": "cell-b5e586fbff84b184",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate a random softmax weight tensor and use it to compute the loss.\n",
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device).double()\n",
    "\n",
    "X_batch = data_dict['X_val'][:128].double()\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to log(10.0).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (math.log(10.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use gradient checking to debug the analytic gradient of our naive softmax loss function. If you've implemented the gradient correctly, you should see relative errors less than `1e-5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eac9e1fc9d03f0cb8d66c1f2060f2cdb",
     "grade": true,
     "grade_id": "cell-1fe9146ce30b4eac",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device).double()\n",
    "X_batch = data_dict['X_val'][:128].double()\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "loss, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg=0.0)[0]\n",
    "grad_check_sparse(f, W, grad, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform another gradient check with regularization enabled. Again you should see relative errors less than `1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c87782da7481bc16a8dcf89c96ba0f70",
     "grade": true,
     "grade_id": "cell-b1190851ac6719f9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device).double()\n",
    "reg = 10.0\n",
    "\n",
    "X_batch = data_dict['X_val'][:128].double()\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "loss, grad = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg)[0]\n",
    "grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation above is fairly inefficient since it uses nested Python loops over the training set.\n",
    "\n",
    "When implementing algorithms in PyTorch, it's best to avoid loops in Python if possible. Instead it is preferable to implement your computation so that all loops happen inside PyTorch functions. This will usually be much faster than writing your own loops in Python, since PyTorch functions can be internally optimized to iterate efficiently, possibly using multiple threads. This is especially important when using a GPU to accelerate your code.\n",
    "\n",
    "The process of eliminating explict loops from your code is called vectorization. Sometimes it is straighforward to vectorize code originally written with loops; other times vectorizing requires thinking about the problem in a new way. We will use vectorization to improve the speed of our distance computation function.\n",
    "\n",
    "Now let's implement a vectorized form of our softmax cross-entropy loss function. The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab46d090ad41bb23800a3854cbf8890",
     "grade": false,
     "grade_id": "cell-b27502f4b61fdb05",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, vectorized version.  When you implement the \n",
    "  regularization over W, please DO NOT multiply the regularization term by 1/2 \n",
    "  (no coefficient). \n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = torch.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. Don't forget the           #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  pass\n",
    "  # END OF YOUR CODE\n",
    "  \n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between the naive and vectorized losses and gradients should both be less than `1e-6`, and your vectorized implementation should be at least 100x faster than the naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6493df914556635dbbff1d09be9af76c",
     "grade": true,
     "grade_id": "cell-a4dac1096d836895",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device)\n",
    "reg = 0.05\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# Run and time the naive version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_naive = 1000.0 * (toc - tic)\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, ms_naive))\n",
    "\n",
    "# Run and time the vectorized version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_vec = 1000.0 * (toc - tic)\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vec, ms_vec))\n",
    "\n",
    "# we use the Frobenius norm to compare the two versions of the gradient.\n",
    "loss_diff = (loss_naive - loss_vec).abs().item()\n",
    "grad_diff = torch.norm(grad_naive - grad_vec, p='fro')\n",
    "print('Loss difference: %.2e' % loss_diff)\n",
    "print('Gradient difference: %.2e' % grad_diff)\n",
    "print('Speedup: %.2fX' % (ms_naive / ms_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an efficient vectorized implementation of the softmax cross-entropy loss and its gradient, we can implement a training pipeline for linear classifiers.\n",
    "\n",
    "Complete the implementation of the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13676d2e4d68136c9bf6a3499b652b37",
     "grade": false,
     "grade_id": "cell-a3fdc0949ade371e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier(loss_func, W, X, y, learning_rate=1e-3, \n",
    "                            reg=1e-5, num_iters=100, batch_size=200, verbose=False):\n",
    "  \"\"\"\n",
    "  Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "  Inputs:\n",
    "  - loss_func: loss function to use when training. It should take W, X, y\n",
    "    and reg as input, and output a tuple of (loss, dW)\n",
    "  - W: A PyTorch tensor of shape (D, K) giving the initial weights of the\n",
    "    classifier. If W is None then it will be initialized here.\n",
    "  - X: A PyTorch tensor of shape (N, D) containing training data; there are N\n",
    "    training samples each of dimension D.\n",
    "  - y: A PyTorch tensor of shape (N,) containing training labels; y[i] = k\n",
    "    means that X[i] has label 0 <= k < K for K classes.\n",
    "  - learning_rate: (float) learning rate for optimization.\n",
    "  - reg: (float) regularization strength.\n",
    "  - num_iters: (integer) number of steps to take when optimizing\n",
    "  - batch_size: (integer) number of training examples to use at each step.\n",
    "  - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "  Returns: A tuple of:\n",
    "  - W: The final value of the weight matrix and the end of optimization\n",
    "  - loss_history: A list of Python scalars giving the values of the loss at each\n",
    "    training iteration.\n",
    "  \"\"\"\n",
    "  # assume y takes values 0...K-1 where K is number of classes\n",
    "  num_train, dim = X.shape\n",
    "  if W is None:\n",
    "    # lazily initialize W\n",
    "    num_classes = torch.max(y) + 1\n",
    "    W = 0.000001 * torch.randn(dim, num_classes, device=X.device, dtype=X.dtype)\n",
    "  else:\n",
    "    num_classes = W.shape[1]\n",
    "  \n",
    "  # Run stochastic gradient descent to optimize W\n",
    "  loss_history = []\n",
    "  for it in range(num_iters):\n",
    "    X_batch = None\n",
    "    y_batch = None\n",
    "    #########################################################################\n",
    "    # TODO:                                                                 #\n",
    "    # Sample batch_size elements from the training data and their           #\n",
    "    # corresponding labels to use in this round of gradient descent.        #\n",
    "    # Store the data in X_batch and their corresponding labels in           #\n",
    "    # y_batch; after sampling, X_batch should have shape (batch_size, dim)  #\n",
    "    # and y_batch should have shape (batch_size,)                           #\n",
    "    #                                                                       #\n",
    "    # Hint: Use torch.randint to generate indices.                          #\n",
    "    #########################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    pass\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    # evaluate loss and gradient\n",
    "    loss, grad = loss_func(W, X_batch, y_batch, reg)\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # perform parameter update\n",
    "    #########################################################################\n",
    "    # TODO:                                                                 #\n",
    "    # Update the weights using the gradient and the learning rate.          #\n",
    "    #########################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    pass\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    if verbose and it % 100 == 0:\n",
    "      print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "  return W, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that your implementation of the softmax loss is numerically stable.\n",
    "\n",
    "If either of the following print nan then you should double-check the numeric stability of your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a2ebbcd0988f3eaf3018286363875ad",
     "grade": true,
     "grade_id": "cell-64c72c82afb95729",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "device = data_dict['X_train'].device\n",
    "dtype = torch.float32\n",
    "D = data_dict['X_train'].shape[1]\n",
    "C = 10\n",
    "\n",
    "W_ones = torch.ones(D, C, device=device, dtype=dtype)\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_naive, W_ones, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-8, reg=2.5e4,\n",
    "                                       num_iters=1, verbose=True)\n",
    "\n",
    "\n",
    "W_ones = torch.ones(D, C, device=device, dtype=dtype)\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, W_ones, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-8, reg=2.5e4,\n",
    "                                       num_iters=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train a softmax classifier with some default hyperparameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed before we perform this operation\n",
    "coutils.utils.fix_random_seed(10)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, None, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-10, reg=2.5e4,\n",
    "                                       num_iters=1500, verbose=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist, 'o')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the prediction stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec147399df573349e30c52a2030b0dc6",
     "grade": false,
     "grade_id": "cell-5c93da7d3d984c48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_linear_classifier(W, X):\n",
    "  \"\"\"\n",
    "  Use the trained weights of this linear classifier to predict labels for\n",
    "  data points.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A PyTorch tensor of shape (D, C), containing weights of a model\n",
    "  - X: A PyTorch tensor of shape (N, D) containing training data; there are N\n",
    "    training samples each of dimension D.\n",
    "\n",
    "  Returns:\n",
    "  - y_pred: PyTorch int64 tensor of shape (N,) giving predicted labels for each\n",
    "    elemment of X. Each element of y_pred should be between 0 and C - 1.\n",
    "  \"\"\"\n",
    "  y_pred = torch.zeros(X.shape[0])\n",
    "  ###########################################################################\n",
    "  # TODO:                                                                   #\n",
    "  # Implement this method. Store the predicted labels in y_pred.            #\n",
    "  ###########################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  pass\n",
    "  # END OF YOUR CODE\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's evaluate the performance our trained model on both the training and validation set. You should see validation accuracy less than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f209a9de34f3e059763ed6f0e251d8b9",
     "grade": true,
     "grade_id": "cell-3af9a3c2735082ed",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the performance on both the training and validation set\n",
    "y_train_pred = predict_linear_classifier(W, data_dict['X_train'])\n",
    "train_acc = 100.0 * (data_dict['y_train'] == y_train_pred).float().mean().item()\n",
    "print('training accuracy: %.2f%%' % train_acc)\n",
    "y_val_pred = predict_linear_classifier(W, data_dict['X_val'])\n",
    "val_acc = 100.0 * (data_dict['y_val'] == y_val_pred).float().mean().item()\n",
    "print('validation accuracy: %.2f%%' % val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the performance of our initial model is quite bad. To find a better hyperparamters, let's first modulize the functions that we've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.W = None\n",
    "    \n",
    "  def train(self, X_train, y_train, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    train_args = (self.loss, self.W, X_train, y_train, learning_rate, reg,\n",
    "                  num_iters, batch_size, verbose)\n",
    "    self.W, loss_history = train_linear_classifier(*train_args)\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    return predict_linear_classifier(self.W, X) \n",
    "  \n",
    "  def loss(self, W, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A PyTorch tensor of shape (D, C) containing (trained) weight of a model.\n",
    "    - X_batch: A PyTorch tensor of shape (N, D) containing a minibatch of N\n",
    "      data points; each point has dimension D.\n",
    "    - y_batch: A PyTorch tensor of shape (N,) containing labels for the minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an tensor of the same shape as W\n",
    "    \"\"\"\n",
    "    pass\n",
    "  def _loss(self, X_batch, y_batch, reg):\n",
    "    self.loss(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "  \n",
    "class Softmax(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "  def loss(self, W, X_batch, y_batch, reg):\n",
    "    return softmax_loss_vectorized(W, X_batch, y_batch, reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths.\n",
    "\n",
    "To get full credit for the assignment your best model found through cross-validation should achieve an accuracy of at least 37% on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e870069350cf15ec7c7631ecc6eab562",
     "grade": false,
     "grade_id": "cell-e27d124b1f9f72f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "learning_rates = [] # learning rate candidates\n",
    "regularization_strengths = [] # regularization strengths candidates\n",
    "\n",
    "# As before, store your cross-validation results in this dictionary.\n",
    "# The keys should be tuples of (learning_rate, regularization_strength) and\n",
    "# the values should be tuples (train_accuracy, val_accuracy)\n",
    "results = {}\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# Save your best trained softmax classifer in best_softmax.                    # \n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a softmax classifier on  #\n",
    "# the training set, compute its accuracy on the training and validation sets,  #\n",
    "# and store these numbers in the results dictionary. In addition, store the    # \n",
    "# best validation accuracy in best_val and the Softmax object that achieves    #\n",
    "# this accuracy in best_softmax.                                              #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the classifiers don't take much time to train;       #\n",
    "# once you are confident that your validation code works, you should rerun the #\n",
    "# validation code with a larger value for num_iters.                           #\n",
    "################################################################################\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "# END OF YOUR CODE\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to visualize your cross-validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2541d0a5fb2ac072d265576d1127f9d3",
     "grade": true,
     "grade_id": "cell-7aaa3354258c3469",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Them, evaluate the performance of your best model on test set. To get full credit for this assignment you should achieve a test-set accuracy above 0.36.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61ec7a4030b1bf33c78ed0c595900dfd",
     "grade": true,
     "grade_id": "cell-7b3168ec5bac1029",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = best_softmax.predict(data_dict['X_test'])\n",
    "test_accuracy = torch.mean((data_dict['y_test'] == y_test_pred).float())\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the learned weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(3, 32, 32, 10)\n",
    "w = w.transpose(0, 2).transpose(1, 0)\n",
    "\n",
    "w_min, w_max = torch.min(w), torch.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "\n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.type(torch.uint8).cpu())\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
